{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation System\n",
    "- baseline estimator\n",
    "- SVD\n",
    "- SVD + bias\n",
    "- SVD + bias + attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    # 读取train.txt格式的数据，返回字典\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line:  # EOF\n",
    "                break\n",
    "            # 读取user_id和rate_num\n",
    "            user_id, rate_num = line.split('|')\n",
    "            rate_num = int(rate_num)\n",
    "            user_id = int(user_id)\n",
    "            # 读取用户的评分数据\n",
    "            rate_data = {}\n",
    "            for i in range(rate_num):\n",
    "                item_id, score = f.readline().strip().split()\n",
    "                item_id = int(item_id)\n",
    "                score = int(score)\n",
    "                rate_data[item_id] = score\n",
    "            # 保存该用户的数据\n",
    "            data[user_id] = rate_data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_data): 19835\n"
     ]
    }
   ],
   "source": [
    "train_path =\"data/train_data.txt\"\n",
    "train_data = read_data(train_path)\n",
    "print(\"len(train_data):\", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "valid_path =\"data/validate_data.txt\"\n",
    "valid_data = read_data(valid_path)\n",
    "print(\"len(valid_data):\", len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### μ : overall mean rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算全局平均分\n",
    "def cal_global_avg(data):\n",
    "    sum_score = 0\n",
    "    sum_num = 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        sum_score += sum(rate_data.values())\n",
    "        sum_num += len(rate_data)\n",
    "    return sum_score / sum_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m global_avg \u001b[38;5;241m=\u001b[39m cal_global_avg(\u001b[43mtrain_data\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_avg:\u001b[39m\u001b[38;5;124m\"\u001b[39m, global_avg)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(train_data)\n",
    "print(\"global_avg:\", global_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_x : rating deviation of user x (ave.rating of user x - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个用户的平均评分，用户偏差\n",
    "def cal_user_bias(data, average_score):\n",
    "    # 每个用户的平均评分\n",
    "    user_average_score = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        total_score = 0\n",
    "        for score in rate_data.values():\n",
    "            total_score += score\n",
    "        user_average_score[user_id] = total_score / len(rate_data)\n",
    "    # 每个用户与全局平均评分的偏差\n",
    "    user_bias = {}\n",
    "    for user_id, u_ave_score in user_average_score.items():\n",
    "        user_bias[user_id] = u_ave_score - average_score\n",
    "    # 最小偏差，最大偏差，平均偏差\n",
    "    max_bias = max(user_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(user_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in user_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(user_bias)\n",
    "    return user_average_score, user_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (547, 50.52937246322808)\n",
      "min_bias: (413, -49.47062753677192)\n",
      "average_bias: 20.37407831847788\n"
     ]
    }
   ],
   "source": [
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b_i : rating deviation of item i (ave.rating of item i - μ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计每个物品的平均评分，物品偏差\n",
    "def cal_item_bias(data, average_score):\n",
    "    # 统计物品得分\n",
    "    item_scores = {}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if item_id in item_scores:\n",
    "                item_scores[item_id].append(score)\n",
    "            else:\n",
    "                item_scores[item_id] = [score]\n",
    "    # 计算物品平均得分\n",
    "    item_average_score = {}\n",
    "    for item_id, scores in item_scores.items():\n",
    "        item_average_score[item_id] = sum(scores) / len(scores)\n",
    "    # 计算物品偏差\n",
    "    item_bias = {}\n",
    "    for item_id, i_ave_score in item_average_score.items():\n",
    "        item_bias[item_id] = i_ave_score - average_score\n",
    "    # 最大偏差，最小偏差，平均偏差\n",
    "    max_bias = max(item_bias.items(), key=lambda x: x[1])\n",
    "    min_bias = min(item_bias.items(), key=lambda x: x[1])\n",
    "    total_bias = 0\n",
    "    for bias in item_bias.values():\n",
    "        total_bias += bias\n",
    "    average_bias = total_bias / len(item_bias)\n",
    "    return item_average_score, item_bias, max_bias, min_bias, average_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_bias: (210761, 50.52937246322808)\n",
      "min_bias: (211658, -49.47062753677192)\n",
      "average_bias: -5.654850049099554\n"
     ]
    }
   ],
   "source": [
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineEstimator:\n",
    "    def __init__(self, global_avg, user_bias, item_bias):\n",
    "        self.global_avg = global_avg\n",
    "        self.user_bias = user_bias\n",
    "        self.item_bias = item_bias\n",
    "        self.baseline_estimator = {}\n",
    "\n",
    "    def save_model(self, path=\"models/baseline_estimator.pkl\"):\n",
    "        # 保存自身模型\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        score = self.global_avg\n",
    "        if user_id in self.user_bias:\n",
    "            score += self.user_bias[user_id]\n",
    "        if item_id in self.item_bias:\n",
    "            score += self.item_bias[item_id]\n",
    "        score = max(0, score)\n",
    "        score = min(100, score)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "40.474727654484916\n",
      "33.31725125110686\n",
      "49.47062753677192\n"
     ]
    }
   ],
   "source": [
    "estimator = BaselineEstimator(global_avg, user_bias, item_bias)\n",
    "print(estimator.predict(1,127640))  # 预测\n",
    "print(estimator.user_bias[1])\n",
    "print(estimator.item_bias[127640])\n",
    "print(estimator.global_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_data: 5\n"
     ]
    }
   ],
   "source": [
    "baseline_data = {\n",
    "    \"global_avg\": global_avg,\n",
    "    \"user_average_score\": user_average_score,\n",
    "    \"user_bias\": user_bias,\n",
    "    \"item_average_score\": item_average_score,\n",
    "    \"item_bias\": item_bias\n",
    "}\n",
    "\n",
    "with open(\"models/baseline_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(baseline_data, f)\n",
    "\n",
    "print(\"baseline_data:\", len(baseline_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(data, model):\n",
    "    rmse, count = 0.0, 0\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            predict = model.predict(user_id, item_id)\n",
    "            rmse += (predict - score) ** 2\n",
    "            count += 1\n",
    "    rmse = np.sqrt((rmse / count))\n",
    "    return rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_rmse: 27.332504852224535\n",
      "baseline_rmse: 29.993169296413154\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/baseline_estimator.pkl\", \"rb\") as f:\n",
    "    estimator = pickle.load(f)\n",
    "\n",
    "baseline_rmse = RMSE(train_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)\n",
    "\n",
    "baseline_rmse = RMSE(valid_data, estimator)\n",
    "print(\"baseline_rmse:\", baseline_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD\n",
    "various SVD models with 50 latent factors:\n",
    "- basic SVD\n",
    "  - time of 1 epoch: 50s\n",
    "  - train RMSE:  17.82015769020438\n",
    "  - valid RMSE:  29.4390823320279\n",
    "- SVD + bias\n",
    "  - time of 1 epoch: 75s\n",
    "  - train RMSE:  16.57224154523797\n",
    "  - valid RMSE:  27.77936363893447\n",
    "- SVD + bias + attributes (k=3 portion=7:3)\n",
    "  - time of 1 epoch: 100s\n",
    "  - train RMSE:  16.812805151033487\n",
    "  - valid RMSE:  27.056434665845632\n",
    "- factors = 200, k = 5, portion = 6:4, lambda = 0.1\n",
    "  - train RMSE:  13.99533877730795\n",
    "  - valid RMSE:  26.50446973316574"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试将分数压缩到0-10之间(没啥用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_data(data):\n",
    "    \"\"\"\n",
    "    压缩数据\n",
    "    Args:\n",
    "        data:数据\n",
    "    Returns:\n",
    "        compressed_data:压缩后的数据\n",
    "    \"\"\"\n",
    "    compressed_data={}\n",
    "    for user_id, rate_data in data.items():\n",
    "        for item_id, score in rate_data.items():\n",
    "            if user_id not in compressed_data:\n",
    "                compressed_data[user_id] = {}\n",
    "            compressed_data[user_id][item_id] = (float)(score/10)\n",
    "    return compressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(compress_train_data): 19835\n",
      "len(compress_valid_data): 19835\n"
     ]
    }
   ],
   "source": [
    "compress_train_data = compress_data(train_data)\n",
    "print(\"len(compress_train_data):\", len(compress_train_data))\n",
    "compress_valid_data = compress_data(valid_data)\n",
    "print(\"len(compress_valid_data):\", len(compress_valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_avg: 4.947062753677194\n",
      "max_bias: (547, 5.052937246322806)\n",
      "min_bias: (413, -4.947062753677194)\n",
      "average_bias: 2.037407831847696\n",
      "max_bias: (210761, 5.052937246322806)\n",
      "min_bias: (211658, -4.947062753677194)\n",
      "average_bias: -0.5654850049047325\n"
     ]
    }
   ],
   "source": [
    "global_avg = cal_global_avg(compress_train_data)\n",
    "print(\"global_avg:\", global_avg)\n",
    "\n",
    "user_average_score, user_bias, max_bias, min_bias, average_bias = cal_user_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)\n",
    "\n",
    "item_average_score, item_bias, max_bias, min_bias, average_bias = cal_item_bias(compress_train_data, global_avg)\n",
    "print(\"max_bias:\", max_bias)\n",
    "print(\"min_bias:\", min_bias)\n",
    "print(\"average_bias:\", average_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义SVD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data = {}\n",
    "with open(\"models/baseline_data.pkl\", \"rb\") as f:\n",
    "    baseline_data = pickle.load(f)\n",
    "# baseline_data[\"global_avg\"] = global_avg\n",
    "# baseline_data[\"user_bias\"] = user_bias\n",
    "# baseline_data[\"item_bias\"] = item_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD:\n",
    "    def __init__(self, baseline_data, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        score = self.global_avg + bx + bi + np.dot(p, q)\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.9\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_model = SVD(baseline_data, factor=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [01:14<00:00, 265.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=27.93013527620241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [01:23<00:00, 238.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=27.79854495676883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [01:16<00:00, 258.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=27.777485680470978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [01:13<00:00, 270.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=27.807101586190697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [01:15<00:00, 264.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=27.856668487707616\n"
     ]
    }
   ],
   "source": [
    "SVD_model.train(5, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.57224154523797\n",
      "valid RMSE:  27.77936363893447\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.062878369561574\n",
      "valid RMSE:  28.68405924929402\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/SVD_50factor.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "train_RMSE = RMSE(train_data, model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the attributes of the items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_attribute:\n",
    "    def __init__(self, baseline_data, similar_nodes, k = 3, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2, \n",
    "                 lambda_bx = 1e-2, lambda_bi = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            baseline_data: dict, baseline数据\n",
    "            similar_nodes: dict, 每个节点的相似节点\n",
    "            k: int, 使用的相似节点个数\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "            lambda_bx: float, 正则化参数\n",
    "            lambda_bi: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        self.lambda_bx = lambda_bx\n",
    "        self.lambda_bi = lambda_bi\n",
    "        # 用户与物品偏置\n",
    "        self.global_avg = baseline_data[\"global_avg\"]\n",
    "        self.bx = baseline_data[\"user_bias\"]\n",
    "        self.bi = baseline_data[\"item_bias\"]\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "        # 相似节点\n",
    "        self.similar_nodes = similar_nodes\n",
    "        self.k = k\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        if user_id in self.bx.keys():\n",
    "            bx = self.bx[user_id]\n",
    "        else:\n",
    "            bx = 0\n",
    "        if item_id in self.bi.keys():\n",
    "            bi = self.bi[item_id]\n",
    "        else:\n",
    "            bi = 0\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        # 直接得分由SVD模型得到\n",
    "        direct_score = self.global_avg + bx + bi + np.dot(p, q)\n",
    "        # indrect_score由相似节点得分平均得到\n",
    "        indirect_score, count = 0, 0\n",
    "        if item_id in self.similar_nodes:\n",
    "            for node_id in self.similar_nodes[item_id]:\n",
    "                temp_q = self.Q[:, node_id]\n",
    "                indirect_score += np.dot(p, temp_q) + self.global_avg + bx\n",
    "                if node_id in self.bi.keys():\n",
    "                    indirect_score += self.bi[node_id]\n",
    "                count += 1\n",
    "                if count == self.k:\n",
    "                    break\n",
    "        if count == 0:\n",
    "            score = direct_score\n",
    "        else:\n",
    "            score = direct_score * 0.6 + (indirect_score / count) * 0.4\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "\n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        loss += self.lambda_bx * np.linalg.norm(list(self.bx.values())) ** 2\n",
    "        loss += self.lambda_bi * np.linalg.norm(list(self.bi.values())) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    bx = self.bx[user_id]\n",
    "                    bi = self.bi[item_id]\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.bx[user_id] += lr * (error - self.lambda_bx * bx)\n",
    "                    self.bi[item_id] += lr * (error - self.lambda_bi * bi)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(similar_nodes): 507172\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/similar_nodes.pkl\", \"rb\") as f:\n",
    "    similar_nodes = pickle.load(f)\n",
    "print(\"len(similar_nodes):\", len(similar_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_attribute_model = SVD_attribute(baseline_data, similar_nodes, k = 5, factor = 200, lambda_bi=1e-1, lambda_bx=1e-1, lambda_p=1e-1, lambda_q=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/19835 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [04:39<00:00, 71.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=29.74155052825229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [03:29<00:00, 94.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=29.260608661309202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [03:24<00:00, 97.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=28.773075826218733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [03:20<00:00, 98.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=28.26915588825707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [03:44<00:00, 88.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=27.862518942243195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19835/19835 [03:32<00:00, 93.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished: validate loss=27.581582139240712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19835/19835 [03:12<00:00, 103.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished: validate loss=27.415648977219295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19835/19835 [02:57<00:00, 111.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished: validate loss=27.332609384701918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19835/19835 [02:51<00:00, 115.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished: validate loss=27.303067738773155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19835/19835 [03:37<00:00, 91.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished: validate loss=27.30618087375584\n"
     ]
    }
   ],
   "source": [
    "SVD_attribute_model.train(10, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  13.99533877730795\n",
      "valid RMSE:  26.50446973316574\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_attribute_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_attribute_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_attribute_200_5_64.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_attribute_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVD_basic:\n",
    "    def __init__(self, factor = 50, lambda_p = 1e-2, lambda_q = 1e-2):\n",
    "        \"\"\"\n",
    "        初始化SVD模型\n",
    "        Args:\n",
    "            factor: int, 隐向量的维度\n",
    "            lambda_p: float, 正则化参数\n",
    "            lambda_q: float, 正则化参数\n",
    "        \"\"\"\n",
    "        self.factor = factor  # 隐向量的维度\n",
    "        # 正则化参数\n",
    "        self.lambda_p = lambda_p\n",
    "        self.lambda_q = lambda_q\n",
    "        # overall max_item_id: 624960 max_user_id: 19834\n",
    "        max_item_id = 624960\n",
    "        max_user_id = 19834\n",
    "        # 随机初始化P(user) Q(item)矩阵\n",
    "        self.P = np.random.normal(0, 0.1, size=(factor, max_user_id + 1))\n",
    "        self.Q = np.random.normal(0, 0.1, size=(factor, max_item_id + 1))\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        \"\"\"\n",
    "        预测用户user对物品item的评分\n",
    "        Args:\n",
    "            user_id: 用户id\n",
    "            item_id: 物品id\n",
    "        Returns:\n",
    "            预测评分\n",
    "        \"\"\"\n",
    "        p = self.P[:, user_id]\n",
    "        q = self.Q[:, item_id]\n",
    "        score = np.dot(p, q)\n",
    "        score = min(score, 100)\n",
    "        score = max(score, 0)\n",
    "        return score\n",
    "    \n",
    "    def loss(self, data):\n",
    "        \"\"\"\n",
    "        计算loss\n",
    "        Args:\n",
    "            data: dict, 训练数据\n",
    "        Returns:\n",
    "            loss\n",
    "        \"\"\"\n",
    "        loss, count = 0.0, 0\n",
    "        for user_id, rate_data in data.items():\n",
    "            for item_id, score in rate_data.items():\n",
    "                predict = self.predict(user_id, item_id)\n",
    "                loss += (predict - score) ** 2\n",
    "                count += 1\n",
    "        # 添加正则化项\n",
    "        loss += self.lambda_p * np.linalg.norm(self.P) ** 2\n",
    "        loss += self.lambda_q * np.linalg.norm(self.Q) ** 2\n",
    "        return np.sqrt(loss / count)\n",
    "\n",
    "    def train(self, epoches, lr, data, valid_data):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        Args:\n",
    "            epoches: int, 迭代次数\n",
    "            lr: float, 学习率\n",
    "            data: dict, 训练数据\n",
    "        \"\"\"\n",
    "        for epoch in range(epoches):\n",
    "            # 使用tqdm显示训练进度\n",
    "            for user_id, rate_data in tqdm(data.items(), desc=\"Epoch {}\".format(epoch)):\n",
    "                for item_id, score in rate_data.items():\n",
    "                    p = self.P[:, user_id]\n",
    "                    q = self.Q[:, item_id]\n",
    "                    # 计算梯度\n",
    "                    error = score - self.predict(user_id, item_id)\n",
    "                    self.P[:, user_id] += lr * (error * q - self.lambda_p * p)\n",
    "                    self.Q[:, item_id] += lr * (error * p - self.lambda_q * q)\n",
    "            # 计算loss\n",
    "            epoch_loss = self.loss(valid_data)\n",
    "            print(\"Epoch {} finished: validate loss={}\".format(epoch, epoch_loss))\n",
    "            # 学习率衰减\n",
    "            lr *= 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 19835/19835 [00:54<00:00, 367.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished: validate loss=39.034601580231886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 19835/19835 [00:51<00:00, 388.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished: validate loss=33.41025828980945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 19835/19835 [00:56<00:00, 352.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished: validate loss=31.63125541116009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 19835/19835 [00:54<00:00, 360.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished: validate loss=30.653772853958973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 19835/19835 [00:57<00:00, 342.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished: validate loss=30.10287798728087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 19835/19835 [00:50<00:00, 395.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished: validate loss=29.813117403437783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 19835/19835 [00:54<00:00, 365.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished: validate loss=29.671782586278294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 19835/19835 [00:55<00:00, 356.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished: validate loss=29.614579967370098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 19835/19835 [00:53<00:00, 368.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished: validate loss=29.602995458974654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 19835/19835 [00:52<00:00, 376.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished: validate loss=29.61521374311972\n"
     ]
    }
   ],
   "source": [
    "SVD_basic_model = SVD_basic(factor=50)\n",
    "SVD_basic_model.train(10, 0.0005, train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RMSE:  16.024254197472054\n",
      "valid RMSE:  29.61382622170249\n"
     ]
    }
   ],
   "source": [
    "train_RMSE = RMSE(train_data, SVD_basic_model)\n",
    "print(\"train RMSE: \", train_RMSE)\n",
    "valid_RMSE = RMSE(valid_data, SVD_basic_model)\n",
    "print(\"valid RMSE: \", valid_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/SVD_basic_50factor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(SVD_basic_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7942438495214534\n",
      "(50, 19835)\n",
      "[-0.71174277 -0.85639551  1.41811758 ...  1.84501671 -0.89567605\n",
      " -0.77031656]\n",
      "(50, 624961)\n",
      "[ 0.0712336   0.00737961 -0.01848777 ... -0.1626112   0.04042626\n",
      "  0.04377546]\n"
     ]
    }
   ],
   "source": [
    "with open(\"models/SVD_basic_50factor.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "print(model.predict(0,0))\n",
    "print(model.P.shape)\n",
    "print(model.P[0])\n",
    "print(model.Q.shape)\n",
    "print(model.Q[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data(path):\n",
    "    '''\n",
    "    读取test.txt格式的数据，返回字典\n",
    "    '''\n",
    "    data = {}\n",
    "    with open(path, 'r') as f:\n",
    "        while True:\n",
    "            line = f.readline().strip()\n",
    "            if not line:\n",
    "                break\n",
    "            user_id, item_num = line.split('|')\n",
    "            item_num = int(item_num)\n",
    "            user_id = int(user_id)\n",
    "            # 读取待预测的物品\n",
    "            item_list = []\n",
    "            for i in range(item_num):\n",
    "                item_id = int(f.readline().strip())\n",
    "                item_list.append(item_id)\n",
    "            data[user_id] = item_list\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_test_data(\"data/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/baseline_estimator.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict done\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for user_id, item_list in test_data.items():\n",
    "    for item_id in item_list:\n",
    "        if results.get(user_id) is None:\n",
    "            results[user_id] = {}\n",
    "        results[user_id][item_id] = model.predict(user_id, item_id)\n",
    "print(\"predict done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write done\n"
     ]
    }
   ],
   "source": [
    "with open(\"./results/result_baseline.txt\", \"w\") as f:\n",
    "    for user_id, rate_data in results.items():\n",
    "        f.write(str(user_id) + \"|\" + str(len(rate_data)) + \"\\n\")\n",
    "        for item_id, score in rate_data.items():\n",
    "            f.write(str(item_id) + \" \" + str(score) + \"\\n\")\n",
    "print(\"write done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
